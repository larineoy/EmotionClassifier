{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries \n",
    "# Keras\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "# sklearn\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Other\n",
    "import librosa\n",
    "import librosa.display\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import IPython.display as ipd  # To play sound in the notebook\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the meta-data file\n",
    "ref = pd.read_csv(\"./Data_path.csv\")  # might need revision for location\n",
    "ref.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this takes a couple of minutes (~10 mins) as we're iterating over 4 datasets \n",
    "df = pd.DataFrame(columns=['feature'])\n",
    "\n",
    "# loop feature extraction over the entire dataset\n",
    "counter=0\n",
    "for index,path in enumerate(ref.path):\n",
    "    X, sample_rate = librosa.load(path\n",
    "                                  , res_type='kaiser_fast'\n",
    "                                  ,duration=2.5\n",
    "                                  ,sr=44100\n",
    "                                  ,offset=0.5\n",
    "                                 )\n",
    "    sample_rate = np.array(sample_rate)\n",
    "    \n",
    "    # mean as the feature. Could do min and max etc as well. \n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X, \n",
    "                                        sr=sample_rate, \n",
    "                                        n_mfcc=13),\n",
    "                    axis=0)\n",
    "    df.loc[counter] = [mfccs]\n",
    "    counter=counter+1   \n",
    "\n",
    "# Check a few records to make sure its processed successfully\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now extract the mean bands to its own feature columns\n",
    "df = pd.concat([ref,pd.DataFrame(df['feature'].values.tolist())],axis=1)\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NA with 0\n",
    "df=df.fillna(0)\n",
    "print(df.shape)\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split between train and test \n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(['path','labels','source'],axis=1)\n",
    "                                                    , df.labels\n",
    "                                                    , test_size=0.25\n",
    "                                                    , shuffle=True\n",
    "                                                    , random_state=42\n",
    "                                                   )\n",
    "\n",
    "# Lets see how the data present itself before normalisation \n",
    "X_train[150:160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lts do data normalization \n",
    "mean = np.mean(X_train, axis=0)\n",
    "std = np.std(X_train, axis=0)\n",
    "\n",
    "X_train = (X_train - mean)/std\n",
    "X_test = (X_test - mean)/std\n",
    "\n",
    "# Check the dataset now \n",
    "X_train[150:160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets few preparation steps to get it into the correct format for Keras \n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# one hot encode the target \n",
    "lb = LabelEncoder()\n",
    "y_train = to_categorical(lb.fit_transform(y_train))\n",
    "y_test = to_categorical(lb.fit_transform(y_test))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(lb.classes_)\n",
    "#print(y_train[0:10])\n",
    "#print(y_test[0:10])\n",
    "\n",
    "# Pickel the lb object for future use \n",
    "filename = 'labels'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(lb,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.expand_dims(X_train, axis=2)\n",
    "X_test = np.expand_dims(X_test, axis=2)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential([\n",
    "    Conv1D(256, kernel_size=3, padding='same', input_shape=(X_train.shape[1],1)),\n",
    "    Activation('relu'),\n",
    "    Conv1D(256, kernel_size=3, padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.3),\n",
    "    Conv1D(128, kernel_size=3, padding='same'),\n",
    "    Activation('relu'),\n",
    "    Conv1D(128, kernel_size=3, padding='same'),\n",
    "    Activation('relu'),\n",
    "    Conv1D(128, kernel_size=3, padding='same'),\n",
    "    Activation('relu'),\n",
    "    Conv1D(128, kernel_size=3, padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.4),\n",
    "    Conv1D(64, kernel_size=3, padding='same'),\n",
    "    Activation('relu'),\n",
    "    Conv1D(64, kernel_size=3, padding='same'),\n",
    "    Activation('relu'),\n",
    "    Flatten(),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(18, activation='relu'),\n",
    "    Activation('softmax')\n",
    "])\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "model_history=model2.fit(X_train, y_train, batch_size=16, epochs=30, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(256, 8, padding='same',input_shape=(X_train.shape[1],1)))  # X_train.shape[1] = No. of Columns\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(256, 8, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "model.add(Conv1D(128, 8, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(128, 8, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(128, 8, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(128, 8, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "model.add(Conv1D(64, 8, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(64, 8, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(18)) # Target class number\n",
    "model.add(Activation('softmax'))\n",
    "# opt = keras.optimizers.SGD(lr=0.0001, momentum=0.0, decay=0.0, nesterov=False)\n",
    "# opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "opt = keras.optimizers.RMSprop(learning_rate=0.00001, decay=1e-6)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "model_history=model.fit(X_train, y_train, batch_size=16, epochs=100, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and weights\n",
    "model_name = 'Emotion_Model.h5'\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Save model and weights at %s ' % model_path)\n",
    "\n",
    "# Save the model to disk\n",
    "model_json = model.to_json()\n",
    "with open(\"model_json.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading json and model architecture \n",
    "json_file = open('model_json.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"saved_models/Emotion_Model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# Keras optimiser\n",
    "opt = keras.optimizers.RMSprop(learning_rate=0.00001, decay=1e-6)\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = loaded_model.predict(X_test, \n",
    "                         batch_size=16, \n",
    "                         verbose=1)\n",
    "\n",
    "preds=preds.argmax(axis=1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions \n",
    "preds = preds.astype(int).flatten()\n",
    "preds = (lb.inverse_transform((preds)))\n",
    "preds = pd.DataFrame({'predictedvalues': preds})\n",
    "\n",
    "# Actual labels\n",
    "actual=y_test.argmax(axis=1)\n",
    "actual = actual.astype(int).flatten()\n",
    "actual = (lb.inverse_transform((actual)))\n",
    "actual = pd.DataFrame({'actualvalues': actual})\n",
    "\n",
    "# Lets combined both of them into a single dataframe\n",
    "finaldf = actual.join(preds)\n",
    "finaldf[170:180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out the predictions to disk\n",
    "finaldf.to_csv('Predictions.csv', index=False)\n",
    "finaldf.groupby('predictedvalues').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the confusion matrix heat map plot\n",
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n",
    "    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    confusion_matrix: numpy.ndarray\n",
    "        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n",
    "        Similarly constructed ndarrays can also be used.\n",
    "    class_names: list\n",
    "        An ordered list of class names, in the order they index the given confusion matrix.\n",
    "    figsize: tuple\n",
    "        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n",
    "        the second determining the vertical size. Defaults to (10,7).\n",
    "    fontsize: int\n",
    "        Font size for axes labels. Defaults to 14.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.figure.Figure\n",
    "        The resulting confusion matrix figure\n",
    "    \"\"\"\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "        \n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Gender recode function\n",
    "def gender(row):\n",
    "    if row == 'female_disgust' or 'female_fear' or 'female_happy' or 'female_sad' or 'female_surprise' or 'female_neutral':\n",
    "        return 'female'\n",
    "    elif row == 'male_angry' or 'male_fear' or 'male_happy' or 'male_sad' or 'male_surprise' or 'male_neutral' or 'male_disgust':\n",
    "        return 'male'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions file \n",
    "finaldf = pd.read_csv(\"Predictions.csv\")\n",
    "classes = finaldf.actualvalues.unique()\n",
    "classes.sort()    \n",
    "\n",
    "# Confusion matrix \n",
    "c = confusion_matrix(finaldf.actualvalues, finaldf.predictedvalues)\n",
    "print(accuracy_score(finaldf.actualvalues, finaldf.predictedvalues))\n",
    "print_confusion_matrix(c, class_names = classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report \n",
    "classes = finaldf.actualvalues.unique()\n",
    "classes.sort()    \n",
    "print(classification_report(finaldf.actualvalues, finaldf.predictedvalues, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modidf = finaldf\n",
    "modidf['actualvalues'] = finaldf.actualvalues.replace({'female_angry':'female'\n",
    "                                       , 'female_disgust':'female'\n",
    "                                       , 'female_fear':'female'\n",
    "                                       , 'female_happy':'female'\n",
    "                                       , 'female_sad':'female'\n",
    "                                       , 'female_surprise':'female'\n",
    "                                       , 'female_neutral':'female'\n",
    "                                       , 'male_angry':'male'\n",
    "                                       , 'male_fear':'male'\n",
    "                                       , 'male_happy':'male'\n",
    "                                       , 'male_sad':'male'\n",
    "                                       , 'male_surprise':'male'\n",
    "                                       , 'male_neutral':'male'\n",
    "                                       , 'male_disgust':'male'\n",
    "                                      })\n",
    "\n",
    "modidf['predictedvalues'] = finaldf.predictedvalues.replace({'female_angry':'female'\n",
    "                                       , 'female_disgust':'female'\n",
    "                                       , 'female_fear':'female'\n",
    "                                       , 'female_happy':'female'\n",
    "                                       , 'female_sad':'female'\n",
    "                                       , 'female_surprise':'female'\n",
    "                                       , 'female_neutral':'female'\n",
    "                                       , 'male_angry':'male'\n",
    "                                       , 'male_fear':'male'\n",
    "                                       , 'male_happy':'male'\n",
    "                                       , 'male_sad':'male'\n",
    "                                       , 'male_surprise':'male'\n",
    "                                       , 'male_neutral':'male'\n",
    "                                       , 'male_disgust':'male'\n",
    "                                      })\n",
    "\n",
    "classes = modidf.actualvalues.unique()  \n",
    "classes.sort() \n",
    "\n",
    "# Confusion matrix \n",
    "c = confusion_matrix(modidf.actualvalues, modidf.predictedvalues)\n",
    "print(accuracy_score(modidf.actualvalues, modidf.predictedvalues))\n",
    "print_confusion_matrix(c, class_names = classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report \n",
    "classes = modidf.actualvalues.unique()\n",
    "classes.sort()    \n",
    "print(classification_report(modidf.actualvalues, modidf.predictedvalues, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report \n",
    "classes = modidf.actualvalues.unique()\n",
    "classes.sort()    \n",
    "print(classification_report(modidf.actualvalues, modidf.predictedvalues, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modidf = pd.read_csv(\"Predictions.csv\")\n",
    "modidf['actualvalues'] = modidf.actualvalues.replace({'female_angry':'angry'\n",
    "                                       , 'female_disgust':'disgust'\n",
    "                                       , 'female_fear':'fear'\n",
    "                                       , 'female_happy':'happy'\n",
    "                                       , 'female_sad':'sad'\n",
    "                                       , 'female_surprise':'surprise'\n",
    "                                       , 'female_neutral':'neutral'\n",
    "                                       , 'male_angry':'angry'\n",
    "                                       , 'male_fear':'fear'\n",
    "                                       , 'male_happy':'happy'\n",
    "                                       , 'male_sad':'sad'\n",
    "                                       , 'male_surprise':'surprise'\n",
    "                                       , 'male_neutral':'neutral'\n",
    "                                       , 'male_disgust':'disgust'\n",
    "                                      })\n",
    "\n",
    "modidf['predictedvalues'] = modidf.predictedvalues.replace({'female_angry':'angry'\n",
    "                                       , 'female_disgust':'disgust'\n",
    "                                       , 'female_fear':'fear'\n",
    "                                       , 'female_happy':'happy'\n",
    "                                       , 'female_sad':'sad'\n",
    "                                       , 'female_surprise':'surprise'\n",
    "                                       , 'female_neutral':'neutral'\n",
    "                                       , 'male_angry':'angry'\n",
    "                                       , 'male_fear':'fear'\n",
    "                                       , 'male_happy':'happy'\n",
    "                                       , 'male_sad':'sad'\n",
    "                                       , 'male_surprise':'surprise'\n",
    "                                       , 'male_neutral':'neutral'\n",
    "                                       , 'male_disgust':'disgust'\n",
    "                                      })\n",
    "\n",
    "classes = modidf.actualvalues.unique() \n",
    "classes.sort() \n",
    "\n",
    "# Confusion matrix \n",
    "c = confusion_matrix(modidf.actualvalues, modidf.predictedvalues)\n",
    "print(accuracy_score(modidf.actualvalues, modidf.predictedvalues))\n",
    "print_confusion_matrix(c, class_names = classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report \n",
    "classes = modidf.actualvalues.unique()\n",
    "classes.sort()    \n",
    "print(classification_report(modidf.actualvalues, modidf.predictedvalues, target_names=classes))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
